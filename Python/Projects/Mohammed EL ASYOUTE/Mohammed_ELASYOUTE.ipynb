{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770d7dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_grid():\n",
    "    return np.full((6, 6), \"*\", dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "471c6780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def place_element():\n",
    "    positions = set()\n",
    "\n",
    "    while len(positions) < 4:\n",
    "        positions.add(tuple(np.random.randint(0, 6, size=2)))\n",
    "\n",
    "    positions = np.array(list(positions))\n",
    "\n",
    "    robot_pos = positions[0]\n",
    "    tresor_pos = positions[1]\n",
    "    pieges_pos = positions[2:4]\n",
    "\n",
    "    return robot_pos, tresor_pos, pieges_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2206af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def render(robot_pos, tresor_pos, pieges_pos):\n",
    "    grid = create_grid()\n",
    "    grid[robot_pos[0], robot_pos[1]] = \"R\"\n",
    "    grid[tresor_pos[0], tresor_pos[1]] = \"T\"\n",
    "    grid[pieges_pos[0, 0], pieges_pos[0, 1]] = \"P\"\n",
    "    grid[pieges_pos[1, 0], pieges_pos[1, 1]] = \"P\"\n",
    "    \n",
    "    for row in grid:\n",
    "        print(\" \".join(row))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e8e4694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def deplace_robot(action, pos):\n",
    "    R = pos.copy()\n",
    "\n",
    "    if action == 0:      # haut\n",
    "        R[0] -= 1\n",
    "    elif action == 1:    # droite\n",
    "        R[1] += 1\n",
    "    elif action == 2:    # bas\n",
    "        R[0] += 1\n",
    "    elif action == 3:    # gauche\n",
    "        R[1] -= 1\n",
    "\n",
    "    R[0] = np.clip(R[0], 0, 5)\n",
    "    R[1] = np.clip(R[1], 0, 5)\n",
    "\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0de4c03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def step(action, robot_pos, tresor_pos, pieges_pos):\n",
    "    robot_pos = deplace_robot(action, robot_pos)\n",
    "\n",
    "    reward = -1\n",
    "    done = False\n",
    "\n",
    "    if np.array_equal(robot_pos, tresor_pos):\n",
    "        reward = 10\n",
    "        done = True\n",
    "    elif any(np.array_equal(robot_pos, p) for p in pieges_pos):\n",
    "        reward = -10\n",
    "        done = True\n",
    "\n",
    "    return robot_pos, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca31c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def jouer_aleatoirement():\n",
    "    robot_pos, tresor_pos, pieges_pos = place_element()\n",
    "    robot_pos = robot_pos.copy()\n",
    "\n",
    "    total_score = 0\n",
    "\n",
    "    for _ in range(50):\n",
    "        action = np.random.randint(0, 4)\n",
    "        robot_pos, reward, done = step(action, robot_pos, tresor_pos, pieges_pos)\n",
    "        total_score += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return total_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2266a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def simulation(nb_parties=200):\n",
    "    scores = [jouer_aleatoirement() for _ in range(nb_parties)]\n",
    "    print(\"Score moyen (al√©atoire):\", np.mean(scores))\n",
    "    print(\"Score max   :\", np.max(scores))\n",
    "    print(\"Score min   :\", np.min(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218b6ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pos_state(robot_pos):\n",
    "    return robot_pos[0] * 6 + robot_pos[1]\n",
    "\n",
    "\n",
    "def epsilon_greedy(robot_pos, Q, epsilon):\n",
    "    state = pos_state(robot_pos)\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(0, 4)\n",
    "    else:\n",
    "        return np.argmax(Q[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "195b4e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# initialisation de variables \n",
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "epsilon = 0.2\n",
    "episodes = 5000 \n",
    "max_steps = 50\n",
    "\n",
    "Q = np.zeros((36, 4))\n",
    "\n",
    "\n",
    "# fonction pour apprentissage du robot\n",
    "def entrainer():\n",
    "    global Q, ROBOT_START, TRESOR_POS, PIEGES_POS\n",
    "    \n",
    "    ROBOT_START, TRESOR_POS, PIEGES_POS = place_element()\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        robot_pos = ROBOT_START.copy()\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            state = pos_state(robot_pos)\n",
    "            action = epsilon_greedy(robot_pos, Q, epsilon)\n",
    "\n",
    "            new_pos, reward, done = step(action, robot_pos, TRESOR_POS, PIEGES_POS)\n",
    "            new_state = pos_state(new_pos)\n",
    "\n",
    "            Q[state, action] += alpha * (reward + gamma * np.max(Q[new_state]) - Q[state, action])\n",
    "\n",
    "            robot_pos = new_pos\n",
    "\n",
    "            if done:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e941cfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# jeu de robot apr√©s apprentissage\n",
    "def jouer_agent(affichage=False, robot_start=None, tresor=None, pieges=None):\n",
    "    \n",
    "    if robot_start is None:\n",
    "        robot_pos = ROBOT_START.copy()\n",
    "        tresor_pos = TRESOR_POS\n",
    "        pieges_pos = PIEGES_POS\n",
    "    else:\n",
    "        robot_pos = robot_start.copy()\n",
    "        tresor_pos = tresor\n",
    "        pieges_pos = pieges\n",
    "\n",
    "    total_score = 0\n",
    "\n",
    "    if affichage:\n",
    "        print(\"√âtat initial:\")\n",
    "        render(robot_pos, tresor_pos, pieges_pos)\n",
    "\n",
    "    for step_num in range(50):\n",
    "        action = np.argmax(Q[pos_state(robot_pos)])\n",
    "        robot_pos, reward, done = step(action, robot_pos, tresor_pos, pieges_pos)\n",
    "        total_score += reward\n",
    "\n",
    "        if affichage:\n",
    "            actions_names = [\"HAUT\", \"DROITE\", \"BAS\", \"GAUCHE\"]\n",
    "            print(f\"Step {step_num + 1}: Action {actions_names[action]}, Reward: {reward}\")\n",
    "            render(robot_pos, tresor_pos, pieges_pos)\n",
    "\n",
    "        if done:\n",
    "            if affichage:\n",
    "                if reward > 0:\n",
    "                    print(\"üéâ Tr√©sor trouv√©!\")\n",
    "                else:\n",
    "                    print(\"üí• Pi√®ge activ√©!\")\n",
    "            break\n",
    "\n",
    "    return total_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daae7a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "# test de jeu du robot apr√©s apprentissage\n",
    "def tester_agent_sur_environnement_fixe(nb_parties=100):\n",
    "    scores = [jouer_agent() for _ in range(nb_parties)]\n",
    "    return scores\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0f7f096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AVANT APPRENTISSAGE ===\n",
      "Performance al√©atoire sur environnements vari√©s:\n",
      "Score moyen (al√©atoire): -24.745\n",
      "Score max   : 10\n",
      "Score min   : -59\n",
      "\n",
      "============================================================\n",
      "APR√àS APPRENTISSAGE\n",
      "============================================================\n",
      "\n",
      "=== TEST 1: Sur l'environnement d'entra√Ænement ===\n",
      "Score moyen: 8.00\n",
      "Score max  : 8\n",
      "Score min  : 8\n",
      "\n",
      "=== TEST 2: D√©monstration visuelle (environnement d'entra√Ænement) ===\n",
      "√âtat initial:\n",
      "* * * * * *\n",
      "R * * T * *\n",
      "P * P * * *\n",
      "* * * * * *\n",
      "* * * * * *\n",
      "* * * * * *\n",
      "\n",
      "Step 1: Action DROITE, Reward: -1\n",
      "* * * * * *\n",
      "* R * T * *\n",
      "P * P * * *\n",
      "* * * * * *\n",
      "* * * * * *\n",
      "* * * * * *\n",
      "\n",
      "Step 2: Action DROITE, Reward: -1\n",
      "* * * * * *\n",
      "* * R T * *\n",
      "P * P * * *\n",
      "* * * * * *\n",
      "* * * * * *\n",
      "* * * * * *\n",
      "\n",
      "Step 3: Action DROITE, Reward: 10\n",
      "* * * * * *\n",
      "* * * T * *\n",
      "P * P * * *\n",
      "* * * * * *\n",
      "* * * * * *\n",
      "* * * * * *\n",
      "\n",
      "üéâ Tr√©sor trouv√©!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# tout le programm pour aficher les r√©sultats\n",
    "\n",
    "def create_grid():\n",
    "    return np.full((6, 6), \"*\", dtype=str)\n",
    "\n",
    "\n",
    "def place_element():\n",
    "    positions = set()\n",
    "\n",
    "    while len(positions) < 4:\n",
    "        positions.add(tuple(np.random.randint(0, 6, size=2)))\n",
    "\n",
    "    positions = np.array(list(positions))\n",
    "\n",
    "    robot_pos = positions[0]\n",
    "    tresor_pos = positions[1]\n",
    "    pieges_pos = positions[2:4]\n",
    "\n",
    "    return robot_pos, tresor_pos, pieges_pos\n",
    "\n",
    "\n",
    "def render(robot_pos, tresor_pos, pieges_pos):\n",
    "    grid = create_grid()\n",
    "    grid[robot_pos[0], robot_pos[1]] = \"R\"\n",
    "    grid[tresor_pos[0], tresor_pos[1]] = \"T\"\n",
    "    grid[pieges_pos[0, 0], pieges_pos[0, 1]] = \"P\"\n",
    "    grid[pieges_pos[1, 0], pieges_pos[1, 1]] = \"P\"\n",
    "    \n",
    "    for row in grid:\n",
    "        print(\" \".join(row))\n",
    "    print()\n",
    "\n",
    "\n",
    "def deplace_robot(action, pos):\n",
    "    R = pos.copy()\n",
    "\n",
    "    if action == 0:      # haut\n",
    "        R[0] -= 1\n",
    "    elif action == 1:    # droite\n",
    "        R[1] += 1\n",
    "    elif action == 2:    # bas\n",
    "        R[0] += 1\n",
    "    elif action == 3:    # gauche\n",
    "        R[1] -= 1\n",
    "\n",
    "    R[0] = np.clip(R[0], 0, 5)\n",
    "    R[1] = np.clip(R[1], 0, 5)\n",
    "\n",
    "    return R\n",
    "\n",
    "\n",
    "def step(action, robot_pos, tresor_pos, pieges_pos):\n",
    "    robot_pos = deplace_robot(action, robot_pos)\n",
    "\n",
    "    reward = -1\n",
    "    done = False\n",
    "\n",
    "    if np.array_equal(robot_pos, tresor_pos):\n",
    "        reward = 10\n",
    "        done = True\n",
    "    elif any(np.array_equal(robot_pos, p) for p in pieges_pos):\n",
    "        reward = -10\n",
    "        done = True\n",
    "\n",
    "    return robot_pos, reward, done\n",
    "\n",
    "\n",
    "def jouer_aleatoirement():\n",
    "    robot_pos, tresor_pos, pieges_pos = place_element()\n",
    "    robot_pos = robot_pos.copy()\n",
    "\n",
    "    total_score = 0\n",
    "\n",
    "    for _ in range(50):\n",
    "        action = np.random.randint(0, 4)\n",
    "        robot_pos, reward, done = step(action, robot_pos, tresor_pos, pieges_pos)\n",
    "        total_score += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return total_score\n",
    "\n",
    "\n",
    "def simulation(nb_parties=200):\n",
    "    scores = [jouer_aleatoirement() for _ in range(nb_parties)]\n",
    "    print(\"Score moyen (al√©atoire):\", np.mean(scores))\n",
    "    print(\"Score max   :\", np.max(scores))\n",
    "    print(\"Score min   :\", np.min(scores))\n",
    "\n",
    "\n",
    "def pos_state(robot_pos):\n",
    "    return robot_pos[0] * 6 + robot_pos[1]\n",
    "\n",
    "\n",
    "def epsilon_greedy(robot_pos, Q, epsilon):\n",
    "    state = pos_state(robot_pos)\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(0, 4)\n",
    "    else:\n",
    "        return np.argmax(Q[state])\n",
    "\n",
    "\n",
    "# Hyperparam√®tres\n",
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "epsilon = 0.2\n",
    "episodes = 5000  # Increased for better learning\n",
    "max_steps = 50\n",
    "\n",
    "Q = np.zeros((36, 4))\n",
    "\n",
    "\n",
    "def entrainer():\n",
    "    global Q, ROBOT_START, TRESOR_POS, PIEGES_POS\n",
    "    \n",
    "    # Create ONE fixed environment for all training episodes\n",
    "    ROBOT_START, TRESOR_POS, PIEGES_POS = place_element()\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        robot_pos = ROBOT_START.copy()\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            state = pos_state(robot_pos)\n",
    "            action = epsilon_greedy(robot_pos, Q, epsilon)\n",
    "\n",
    "            new_pos, reward, done = step(action, robot_pos, TRESOR_POS, PIEGES_POS)\n",
    "            new_state = pos_state(new_pos)\n",
    "\n",
    "            # Q-learning update\n",
    "            Q[state, action] += alpha * (reward + gamma * np.max(Q[new_state]) - Q[state, action])\n",
    "\n",
    "            robot_pos = new_pos\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "def jouer_agent(affichage=False, robot_start=None, tresor=None, pieges=None):\n",
    "    \n",
    "    if robot_start is None:\n",
    "        robot_pos = ROBOT_START.copy()\n",
    "        tresor_pos = TRESOR_POS\n",
    "        pieges_pos = PIEGES_POS\n",
    "    else:\n",
    "        robot_pos = robot_start.copy()\n",
    "        tresor_pos = tresor\n",
    "        pieges_pos = pieges\n",
    "\n",
    "    total_score = 0\n",
    "\n",
    "    if affichage:\n",
    "        print(\"√âtat initial:\")\n",
    "        render(robot_pos, tresor_pos, pieges_pos)\n",
    "\n",
    "    for step_num in range(50):\n",
    "        action = np.argmax(Q[pos_state(robot_pos)])\n",
    "        robot_pos, reward, done = step(action, robot_pos, tresor_pos, pieges_pos)\n",
    "        total_score += reward\n",
    "\n",
    "        if affichage:\n",
    "            actions_names = [\"HAUT\", \"DROITE\", \"BAS\", \"GAUCHE\"]\n",
    "            print(f\"Step {step_num + 1}: Action {actions_names[action]}, Reward: {reward}\")\n",
    "            render(robot_pos, tresor_pos, pieges_pos)\n",
    "\n",
    "        if done:\n",
    "            if affichage:\n",
    "                if reward > 0:\n",
    "                    print(\"üéâ Tr√©sor trouv√©!\")\n",
    "                else:\n",
    "                    print(\"üí• Pi√®ge activ√©!\")\n",
    "            break\n",
    "\n",
    "    return total_score\n",
    "\n",
    "\n",
    "def tester_agent_sur_environnement_fixe(nb_parties=100):\n",
    "    scores = [jouer_agent() for _ in range(nb_parties)]\n",
    "    return scores\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n=== AVANT APPRENTISSAGE ===\")\n",
    "print(\"Performance al√©atoire sur environnements vari√©s:\")\n",
    "simulation(200)\n",
    "\n",
    "entrainer()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"APR√àS APPRENTISSAGE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n=== TEST 1: Sur l'environnement d'entra√Ænement ===\")\n",
    "scores_entrainement = tester_agent_sur_environnement_fixe(200)\n",
    "print(f\"Score moyen: {np.mean(scores_entrainement):.2f}\")\n",
    "print(f\"Score max  : {np.max(scores_entrainement)}\")\n",
    "print(f\"Score min  : {np.min(scores_entrainement)}\")\n",
    "\n",
    "print(\"\\n=== TEST 2: D√©monstration visuelle (environnement d'entra√Ænement) ===\")\n",
    "jouer_agent(affichage=True)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
